{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simple Tutorial On Word2Vec Implementation with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense) \n",
    "from keras.preprocessing.text import (one_hot, Tokenizer)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:97: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, input_dim=7, activation=\"linear\", kernel_initializer=\"uniform\")`\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:98: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(7, activation=\"softmax\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f6c4fc6170f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m#toOneHotEncoding(4, 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateFeaturesAndLabelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_2_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateNeuralNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-f6c4fc6170f2>\u001b[0m in \u001b[0;36mcreateNeuralNetModel\u001b[0;34m(vocab_dim, embedding_dim)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;31m#model.add(Dense(1, init='uniform', activation='sigmoid'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   3147\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3148\u001b[0m     \"\"\"\n\u001b[0;32m-> 3149\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "# To ensure repeatable outcomes\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "def getWord2IntAndInt2Word(corpus):\n",
    "    \"\"\"\n",
    "    Converts the words in a corpus to integers and integers to words. It also provides the vocabulary size\n",
    "    # Arguments:\n",
    "        corpus: The entire corpus\n",
    "    # Returns:\n",
    "        V: Vocabulary size\n",
    "        word_2_int: integer code for a word in the corpus\n",
    "        int_2_word: word representation of an int\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for word in corpus.split():\n",
    "        if word != '.':\n",
    "            words.append(word)\n",
    "    words = set(words)\n",
    "    word_2_int = {}\n",
    "    int_2_word = {}\n",
    "    \n",
    "    for i, token in enumerate(words):\n",
    "        word_2_int[token] = i\n",
    "        int_2_word[i] = token\n",
    "    V = len(words)\n",
    "    return V, word_2_int, int_2_word\n",
    "\n",
    "def corpus2ListOfSentences(corpus):\n",
    "    \"\"\"\n",
    "    Converts a corpus to a list of senstences, with each word in the sentence delimited by a comma\n",
    "    # Arguments:\n",
    "        corpus: The ntire corpus\n",
    "    Returns:\n",
    "        sentence_list: List of senstences\n",
    "    \"\"\"\n",
    "    sentence_list = []\n",
    "    sentences = corpus.split('.')\n",
    "    for word in sentences:\n",
    "        sentence_list.append(word.split())\n",
    "    return sentence_list\n",
    "\n",
    "def createListOfWordAndItsNeighbors(sentences, window=2):\n",
    "    \"\"\"\n",
    "    Creates a list of a word and its neighboring words\n",
    "    # Arguments:\n",
    "        sentences: list of senstences\n",
    "        window: window size for the word and its neighboring words\n",
    "    # Returns:\n",
    "        data: list of a word and its neighboring words\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        for i, word in enumerate(sentence):\n",
    "            for neighbor_word in sentence[max(i - window, 0): min(i + window, len(sentence) + 1)]:\n",
    "                if word != neighbor_word:\n",
    "                    data.append([word, neighbor_word])\n",
    "    return data\n",
    "\n",
    "def toOneHotEncoding(data_index, size):\n",
    "    \"\"\"\n",
    "    Converts a data point to a one-hot-encoding vector of a specified size\n",
    "    # Argumnets:\n",
    "        data_index: data point thta needs to be encoded\n",
    "        size: size of one-hot-encoding vector\n",
    "    # Returns:\n",
    "        vec: returned one-hoteencoded vector\n",
    "    \"\"\"\n",
    "    vec = np.zeros(size)\n",
    "    vec[data_index] = 1\n",
    "    return vec\n",
    "\n",
    "def createFeaturesAndLabelData(context_and_target_words, word_2_int, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates the context and target data as one-hot-encoded array data sets\n",
    "    # Arguments:\n",
    "        context_and_target_words: list of a word and its neighboring words\n",
    "        word_2_int: mapping of words to integers\n",
    "        vocab_size: vocabulary size\n",
    "    # Returns:\n",
    "        X: features\n",
    "        y: labels\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for data in context_and_target_words:\n",
    "        X.append(toOneHotEncoding(word_2_int[data[0]], vocab_size))\n",
    "        y.append(toOneHotEncoding(word_2_int[data[1]], vocab_size))\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X, y\n",
    "\n",
    "def createNeuralNetModel(vocab_dim, embedding_dim):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(embedding_dim, input_dim=vocab_dim, init='uniform', activation='linear'))\n",
    "    model.add(Dense(vocab_dim, init='uniform', activation='softmax'))\n",
    "    #model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def computeEmbeddingsViaNeuralNet(X, y, vocab_dim, embedding_dim):\n",
    "    model = createNeuralNetModel(vocab_dim, embedding_dim)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=150, batch_size=10)\n",
    "    scores = model.evaluate(X, Y)\n",
    "    print(\"{0} {1:.2f}\".format(model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "embedding_dim = 5   \n",
    "vocab_size, word_2_int, int_2_word = getWord2IntAndInt2Word(corpus_raw)\n",
    "sentences = corpus2ListOfSentences(corpus_raw)\n",
    "data = createListOfWordAndItsNeighbors(sentences)\n",
    "#toOneHotEncoding(4, 10)\n",
    "X, y = createFeaturesAndLabelData(data, word_2_int, vocab_size)\n",
    "model = createNeuralNetModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 2, 3, 4, 5, 6, 7]], 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V\n",
    "\n",
    "corpus = [\"I like playing football with my friends\"]\n",
    "tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user-defined parameters\n",
    "corpus = [\"I like playing football with my friends\"] #our example text corpus\n",
    "N = 2 #assume that the hidden layer has dimensionality = 2\n",
    "window_size = 2 #symmetrical\n",
    "eta = 0.1 #learning rate\n",
    "corpus_tokenized, V = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/95/274190b39950e1e9eef4b071acefea832ac3e2c19bb4b442fa54f3214d2e/tensorflow-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (51.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 51.1MB 11kB/s eta 0:00:01  2% |█                               | 1.5MB 2.4MB/s eta 0:00:21    3% |█▎                              | 2.0MB 2.4MB/s eta 0:00:21    11% |███▊                            | 6.0MB 2.6MB/s eta 0:00:18    13% |████▏                           | 6.7MB 1.7MB/s eta 0:00:26    18% |█████▊                          | 9.2MB 1.6MB/s eta 0:00:26    28% |█████████▎                      | 14.8MB 2.1MB/s eta 0:00:18    30% |█████████▊                      | 15.6MB 561kB/s eta 0:01:04    33% |██████████▉                     | 17.3MB 375kB/s eta 0:01:31    37% |███████████▉                    | 19.0MB 1.1MB/s eta 0:00:30    39% |████████████▊                   | 20.4MB 1.6MB/s eta 0:00:19    43% |██████████████                  | 22.3MB 1.8MB/s eta 0:00:16    49% |███████████████▊                | 25.2MB 1.4MB/s eta 0:00:19    50% |████████████████▏               | 25.8MB 2.6MB/s eta 0:00:10    51% |████████████████▋               | 26.5MB 1.5MB/s eta 0:00:17    55% |█████████████████▉              | 28.4MB 2.3MB/s eta 0:00:10    78% |█████████████████████████▏      | 40.2MB 1.9MB/s eta 0:00:06    86% |███████████████████████████▋    | 44.2MB 1.5MB/s eta 0:00:05    93% |█████████████████████████████▉  | 47.7MB 3.4MB/s eta 0:00:02    96% |███████████████████████████████ | 49.4MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: numpy>=1.13.3 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorflow) (1.14.5)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.10.0,>=1.9.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 929kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting setuptools<=39.1.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
      "\u001b[K    100% |████████████████████████████████| 573kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: protobuf>=3.4.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement not upgraded as not directly required: grpcio>=1.8.6 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorflow) (1.13.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/8d/6664518f9b6ced0aa41cf50b989740909261d4c212557400c48e5cda0804/absl-py-0.2.2.tar.gz (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Requirement not upgraded as not directly required: wheel>=0.26 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorflow) (0.30.0)\n",
      "Requirement not upgraded as not directly required: six>=1.10.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.10.0,>=1.9.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: werkzeug>=0.11.10 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (0.12.2)\n",
      "Building wheels for collected packages: absl-py, gast, termcolor\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/a0/f8/e9/1933dbb3447ea6ef57062fd5461cb118deb8c2ed074e8344bf\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast termcolor\n",
      "Installing collected packages: astor, markdown, tensorboard, setuptools, absl-py, gast, termcolor, tensorflow\n",
      "  Found existing installation: setuptools 40.0.0\n",
      "    Uninstalling setuptools-40.0.0:\n",
      "      Successfully uninstalled setuptools-40.0.0\n",
      "  Found existing installation: tensorflow 1.1.0\n",
      "    Uninstalling tensorflow-1.1.0:\n",
      "      Successfully uninstalled tensorflow-1.1.0\n",
      "Successfully installed absl-py-0.2.2 astor-0.7.1 gast-0.2.0 markdown-2.6.11 setuptools-39.1.0 tensorboard-1.9.0 tensorflow-1.9.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install pip --upgrade\n",
    "!pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
