{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simple Tutorial On Word2vec implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2, 3, 4, 5, 6], [1, 7, 8, 9, 10, 11, 1, 12]], 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    \"\"\"Tokenize the corpus text.\n",
    "    :param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\n",
    "    :return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus (the example above would return [[1, 2, 3, 4]])\n",
    "    :return V: size of vocabulary\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V\n",
    "\n",
    "#Test\n",
    "tokenize(['I am coming home tomorrow', 'The dog like to run after the ball'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the One-hot-Encoding representation of the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " center word = [1. 0. 0. 0. 0. 0. 0.] \n",
      " context words =\n",
      " [[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]]]\n",
      "1 \n",
      " center word = [0. 1. 0. 0. 0. 0. 0.] \n",
      " context words =\n",
      " [[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]]]\n",
      "2 \n",
      " center word = [0. 0. 1. 0. 0. 0. 0.] \n",
      " context words =\n",
      " [[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]]]\n",
      "3 \n",
      " center word = [0. 0. 0. 1. 0. 0. 0.] \n",
      " context words =\n",
      " [[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]]]\n",
      "4 \n",
      " center word = [0. 0. 0. 0. 1. 0. 0.] \n",
      " context words =\n",
      " [[[0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "5 \n",
      " center word = [0. 0. 0. 0. 0. 1. 0.] \n",
      " context words =\n",
      " [[[0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "6 \n",
      " center word = [0. 0. 0. 0. 0. 0. 1.] \n",
      " context words =\n",
      " [[[0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    \"\"\"Converts corpus text into context and center words\n",
    "    # Arguments\n",
    "        corpus_tokenized: corpus text\n",
    "        window_size: size of context window\n",
    "    # Returns\n",
    "        context and center words (arrays)\n",
    "    \"\"\"\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i]-1 for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word-1)\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y.ravel())\n",
    "\n",
    "def testOneHotEncoding():\n",
    "    window_size = 2\n",
    "    corpus = [\"I like playing football with my friends\"]\n",
    "    corpus_tokenized, V = tokenize(corpus)\n",
    "    for i, (x, y) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "        print(i, \"\\n center word =\", y, \"\\n context words =\\n\",x)\n",
    "\n",
    "testOneHotEncoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculate softmax based probability for given input vector\n",
    "    # Arguments\n",
    "        x: numpy array/list\n",
    "    # Returns\n",
    "        softmax of input array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continious-Bag-Of-Words Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape: (7, 2)\n",
      "context.shape: (2, 7)\n",
      "Training example #0 \n",
      "-------------------- \n",
      "\n",
      " \t label = [1. 0. 0. 0. 0. 0. 0.], \n",
      " \t context = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\t W1 = [[ 0.54340494  0.27836939]\n",
      " [ 0.40739238  0.86832668]\n",
      " [-0.01240636  0.14511967]\n",
      " [ 0.67074908  0.82585276]\n",
      " [ 0.13670659  0.57509333]\n",
      " [ 0.89132195  0.20920212]\n",
      " [ 0.18532822  0.10837689]]\n",
      "\t W2 = [[2.37522044e-01 9.74588882e-01 8.08598345e-01 1.69452859e-01\n",
      "  8.13081663e-01 2.71730692e-01 4.28973634e-01]\n",
      " [9.80158449e-01 8.08565553e-01 3.29167094e-01 1.69808842e-01\n",
      "  3.65755980e-01 4.13558689e-04 2.46279033e-01]] \n",
      "\t loss = 1.7750419435896188\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (3, 7)\n",
      "Training example #1 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 1. 0. 0. 0. 0. 0.], \n",
      " \t context = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "\t W1 = [[5.56639604e-01 2.89686158e-01]\n",
      " [4.07392378e-01 8.68326677e-01]\n",
      " [8.28305801e-04 1.56436438e-01]\n",
      " [6.83983747e-01 8.37169528e-01]\n",
      " [1.36706590e-01 5.75093329e-01]\n",
      " [8.91321954e-01 2.09202122e-01]\n",
      " [1.85328220e-01 1.08376890e-01]]\n",
      "\t W2 = [[ 0.23121195  1.00675437  0.80254985  0.16507125  0.80692926  0.26747677\n",
      "   0.42395468]\n",
      " [ 0.97359845  0.84200492  0.32287905  0.16525371  0.35935991 -0.00400884\n",
      "   0.24106131]] \n",
      "\t loss = 3.399429331549594\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (4, 7)\n",
      "Training example #2 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 1. 0. 0. 0. 0.], \n",
      " \t context = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "\t W1 = [[5.61872952e-01 2.85249824e-01]\n",
      " [4.12625726e-01 8.63890342e-01]\n",
      " [8.28305801e-04 1.56436438e-01]\n",
      " [6.89217095e-01 8.32733194e-01]\n",
      " [1.41939938e-01 5.70656995e-01]\n",
      " [8.91321954e-01 2.09202122e-01]\n",
      " [1.85328220e-01 1.08376890e-01]]\n",
      "\t W2 = [[ 0.22354561  0.996797    0.84065603  0.16064342  0.80024989  0.26331958\n",
      "   0.41873659]\n",
      " [ 0.96255774  0.82766477  0.37775784  0.15887694  0.3497406  -0.00999584\n",
      "   0.23354646]] \n",
      "\t loss = 5.323941178808285\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (4, 7)\n",
      "Training example #3 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 1. 0. 0. 0.], \n",
      " \t context = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00966035  0.14854727]\n",
      " [ 0.6892171   0.83273319]\n",
      " [ 0.13145128  0.56276783]\n",
      " [ 0.88083329  0.20131295]\n",
      " [ 0.18532822  0.10837689]]\n",
      "\t W2 = [[ 0.21775807  0.98959277  0.83509537  0.19287099  0.79483857  0.25952945\n",
      "   0.4142629 ]\n",
      " [ 0.95535615  0.81870036  0.37083857  0.19897854  0.34300715 -0.01471198\n",
      "   0.22797972]] \n",
      "\t loss = 7.540849570192512\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (4, 7)\n",
      "Training example #4 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 0. 1. 0. 0.], \n",
      " \t context = [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.69455676  0.82980761]\n",
      " [ 0.13145128  0.56276783]\n",
      " [ 0.88617296  0.19838737]\n",
      " [ 0.19066788  0.1054513 ]]\n",
      "\t W2 = [[ 0.21139714  0.98106847  0.82819932  0.18794169  0.83176617  0.25479284\n",
      "   0.40878249]\n",
      " [ 0.9506522   0.81239659  0.36573889  0.19533328  0.37031536 -0.01821474\n",
      "   0.22392692]] \n",
      "\t loss = 9.412493718198514\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (3, 7)\n",
      "Training example #5 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 0. 0. 1. 0.], \n",
      " \t context = [[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.12071664  0.54631869]\n",
      " [ 0.88617296  0.19838737]\n",
      " [ 0.17993324  0.08900216]]\n",
      "\t W2 = [[0.2058461  0.97434378 0.82309063 0.18416489 0.82663959 0.28520946\n",
      "  0.40465366]\n",
      " [0.94247299 0.80248806 0.35821146 0.18976835 0.36276158 0.0266028\n",
      "  0.21784328]] \n",
      "\t loss = 11.69069335502739\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (2, 7)\n",
      "Training example #6 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 0. 0. 0. 1.], \n",
      " \t context = [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900216]]\n",
      "\t W2 = [[0.19857939 0.96418767 0.81511412 0.17873393 0.81863527 0.27983192\n",
      "  0.44886581]\n",
      " [0.93709845 0.7949765  0.35231195 0.18575155 0.3568415  0.02262551\n",
      "  0.25054305]] \n",
      "\t loss = 13.796008271496845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cbow(context, label, W1, W2, loss, eta):\n",
    "    \"\"\"\n",
    "    Implementation of Continuous-Bag-of-Words Word2Vec model\n",
    "    :param context: all the context words (these represent the inputs)\n",
    "    :param label: the center word (this represents the label)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :param eta; Learning rate\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    x = np.mean(context, axis=0)\n",
    "    h = np.dot(W1.T, x)\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "    e = -label + y_pred\n",
    "    dW2 = np.outer(h, e)\n",
    "    dW1 = np.outer(x, np.dot(W2, e))\n",
    "    new_W1 = W1 - eta * dW1\n",
    "    new_W2 = W2 - eta * dW2\n",
    "    loss += -float(u[label == 1]) + np.log(np.sum(np.exp(u)))\n",
    "    return new_W1, new_W2, loss\n",
    "\n",
    "def testCbowImplementation():\n",
    "    #user-defined parameters\n",
    "    corpus = [\"I like playing football with my friends\"] #our example text corpus\n",
    "    N = 2 #assume that the hidden layer has dimensionality = 2\n",
    "    window_size = 2 #symmetrical\n",
    "    eta = 0.1 #learning rate\n",
    "    corpus_tokenized, V = tokenize(corpus)\n",
    "    #initialize weights (with random values) and loss function\n",
    "    np.random.seed(100)\n",
    "    W1 = np.random.rand(V, N)\n",
    "    W2 = np.random.rand(N, V)\n",
    "    loss = 0.\n",
    "    for i, (context, label) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "        print(\"W1.shape: {}\".format(W1.shape))\n",
    "        _, x_row, x_col = context.shape\n",
    "        context = context.reshape((x_row, x_col))\n",
    "        print(\"context.shape: {}\".format(context.shape))\n",
    "        W1, W2, loss = cbow(context, label, W1, W2, loss, eta)\n",
    "        print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t context = {}\".format(i, label, context))\n",
    "        print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))\n",
    "        \n",
    "testCbowImplementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape: (7, 2)\n",
      "context.shape: (2, 7)\n",
      "Training example #0 \n",
      "-------------------- \n",
      "\n",
      " \t label = [1. 0. 0. 0. 0. 0. 0.], \n",
      " \t context = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.42451759 0.84477613]\n",
      " [0.00471886 0.12156912]\n",
      " [0.67074908 0.82585276]\n",
      " [0.13670659 0.57509333]\n",
      " [0.89132195 0.20920212]\n",
      " [0.18532822 0.10837689]]\n",
      "\t W2 = [[ 2.04840301e-01  1.01127493e+00  8.48700217e-01  1.60239872e-01\n",
      "   7.98680093e-01  2.62275625e-01  4.17937082e-01]\n",
      " [ 9.32418944e-01  8.34375540e-01  3.55074638e-01  1.69416325e-01\n",
      "   3.63844467e-01 -3.55302092e-04  2.45373897e-01]] \n",
      "\t loss = 3.447952656315579\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (3, 7)\n",
      "Training example #1 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 1. 0. 0. 0. 0. 0.], \n",
      " \t context = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.36581851 0.83603688]\n",
      " [0.00471886 0.12156912]\n",
      " [0.67074908 0.82585276]\n",
      " [0.13670659 0.57509333]\n",
      " [0.89132195 0.20920212]\n",
      " [0.18532822 0.10837689]]\n",
      "\t W2 = [[ 0.22410696  0.98121988  0.87244095  0.19075025  0.78022598  0.25147173\n",
      "   0.40373237]\n",
      " [ 0.97075897  0.77456697  0.40231792  0.23013098  0.32712139 -0.0218547\n",
      "   0.21710698]] \n",
      "\t loss = 9.436262037080558\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (4, 7)\n",
      "Training example #2 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 1. 0. 0. 0. 0.], \n",
      " \t context = [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.36581851 0.83603688]\n",
      " [0.0093349  0.18107647]\n",
      " [0.67074908 0.82585276]\n",
      " [0.13670659 0.57509333]\n",
      " [0.89132195 0.20920212]\n",
      " [0.18532822 0.10837689]]\n",
      "\t W2 = [[ 0.22429096  0.98140966  0.87217146  0.19095908  0.78043095  0.25121654\n",
      "   0.40346947]\n",
      " [ 0.97549928  0.77945616  0.39537533  0.23551095  0.33240188 -0.02842907\n",
      "   0.21033396]] \n",
      "\t loss = 17.14424509144889\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (4, 7)\n",
      "Training example #3 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 1. 0. 0. 0.], \n",
      " \t context = [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.36581851 0.83603688]\n",
      " [0.0093349  0.18107647]\n",
      " [0.71321469 0.76832138]\n",
      " [0.13670659 0.57509333]\n",
      " [0.89132195 0.20920212]\n",
      " [0.18532822 0.10837689]]\n",
      "\t W2 = [[0.17801755 0.98308593 0.89498839 0.16639983 0.80799822 0.29772768\n",
      "  0.37573052]\n",
      " [0.91852564 0.78152006 0.42346843 0.20527263 0.36634379 0.02883728\n",
      "  0.17618068]] \n",
      "\t loss = 24.842100684558446\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (4, 7)\n",
      "Training example #4 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 0. 1. 0. 0.], \n",
      " \t context = [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.36581851 0.83603688]\n",
      " [0.0093349  0.18107647]\n",
      " [0.71321469 0.76832138]\n",
      " [0.08725378 0.46990463]\n",
      " [0.89132195 0.20920212]\n",
      " [0.18532822 0.10837689]]\n",
      "\t W2 = [[0.16824777 0.97300585 0.90055305 0.17359827 0.80024692 0.30544468\n",
      "  0.38285157]\n",
      " [0.87742641 0.73911547 0.44687768 0.23555483 0.33373585 0.06130095\n",
      "  0.20613731]] \n",
      "\t loss = 33.22475207854099\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (3, 7)\n",
      "Training example #5 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 0. 0. 1. 0.], \n",
      " \t context = [[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.36581851 0.83603688]\n",
      " [0.0093349  0.18107647]\n",
      " [0.71321469 0.76832138]\n",
      " [0.08725378 0.46990463]\n",
      " [0.83823393 0.15174941]\n",
      " [0.18532822 0.10837689]]\n",
      "\t W2 = [[0.13917283 0.91513346 0.84951738 0.23718742 0.84379997 0.2777448\n",
      "  0.44139225]\n",
      " [0.87060223 0.72553225 0.4348991  0.25047984 0.34395819 0.05479952\n",
      "  0.21987739]] \n",
      "\t loss = 39.51041290437907\n",
      "\n",
      "W1.shape: (7, 2)\n",
      "context.shape: (2, 7)\n",
      "Training example #6 \n",
      "-------------------- \n",
      "\n",
      " \t label = [0. 0. 0. 0. 0. 0. 1.], \n",
      " \t context = [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\t W1 = [[0.60454916 0.30236044]\n",
      " [0.36581851 0.83603688]\n",
      " [0.0093349  0.18107647]\n",
      " [0.71321469 0.76832138]\n",
      " [0.08725378 0.46990463]\n",
      " [0.83823393 0.15174941]\n",
      " [0.18788028 0.06339965]]\n",
      "\t W2 = [[0.13400901 0.90926401 0.84389866 0.23227074 0.85677507 0.29142779\n",
      "  0.43630284]\n",
      " [0.86758251 0.72209989 0.43161337 0.24760464 0.35154581 0.0628011\n",
      "  0.21690119]] \n",
      "\t loss = 43.44166218899798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def skipgram(context, x, W1, W2, loss, eta):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram Word2Vec model\n",
    "    :param context: all the context words (these represent the labels)\n",
    "    :param label: the center word (this represents the input)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    h = np.dot(W1.T, x)\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "    e = np.array([-label + y_pred.T for label in context])\n",
    "    dW2 = np.outer(h, np.sum(e, axis=0))\n",
    "    dW1 = np.outer(x, np.dot(W2, np.sum(e, axis=0)))\n",
    "    new_W1 = W1 - eta * dW1\n",
    "    new_W2 = W2 - eta * dW2\n",
    "    loss += -np.sum([u[label == 1] for label in context]) + len(context) * np.log(np.sum(np.exp(u)))\n",
    "    return new_W1, new_W2, loss\n",
    "\n",
    "def testSkipGramImplementation():\n",
    "    #user-defined parameters\n",
    "    corpus = [\"I like playing football with my friends\"] #our example text corpus\n",
    "    N = 2 #assume that the hidden layer has dimensionality = 2\n",
    "    window_size = 2 #symmetrical\n",
    "    eta = 0.1 #learning rate\n",
    "    corpus_tokenized, V = tokenize(corpus)\n",
    "    #initialize weights (with random values) and loss function\n",
    "    np.random.seed(100)\n",
    "    W1 = np.random.rand(V, N)\n",
    "    W2 = np.random.rand(N, V)\n",
    "    loss = 0.\n",
    "    for i, (context, label) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "        print(\"W1.shape: {}\".format(W1.shape))\n",
    "        _, x_row, x_col = context.shape\n",
    "        context = context.reshape((x_row, x_col))\n",
    "        print(\"context.shape: {}\".format(context.shape))\n",
    "        W1, W2, loss = skipgram(context, label, W1, W2, loss, eta)\n",
    "        print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t context = {}\".format(i, label, context))\n",
    "        print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))\n",
    "        \n",
    "testSkipGramImplementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l[-1] = 9\tlen(l) = 10\n",
      "ll = [0 1 2 3 4 5 6 7 8 9]\n",
      "ll.ravel() = [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "l = list(range(10))\n",
    "ll = np.array(l, 'int')\n",
    "print(\"l[-1] = {0}\\tlen(l) = {1}\".format(l[-1], len(l)))\n",
    "print(\"ll = {0}\\nll.ravel() = {1}\".format(ll, ll.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
