{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Simple Demo of Gym for RL"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Install Gym from OpenAI"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#!pip install gym==0.7.4",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Define the required imports"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import gym\nimport numpy as np\nenv = gym.make('FrozenLake-v0')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Let us see the GYM default settings"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"observation_space\")\nprint(env.observation_space)\nprint(\"action_space\")\nprint(env.action_space)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "observation_space\nDiscrete(16)\naction_space\nDiscrete(4)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Apply a random search of the Grid to reach the goal\n- Observation, in this case, represents the location on the grid. \n- Reward is the amount of reward the step generated. \n- Done is a Boolean value indicating whether the simulation ended\n- Info is primarily used for diagnostic purposes"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def randomExploreGrid(is_print_status=True):\n    env.reset()\n    done = False\n    while done == False:\n        action = env.action_space.sample()\n        observation, reward, done, info = env.step(action)\n        if is_print_status:\n            print(action, observation, reward, done, info)\n        if(reward > 0):\n            print(\"success\")\n            print(reward)\nrandomExploreGrid()\n#print(\"env.reset:\\n{}\".format(env.reset()))",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "3 1 0.0 False {'prob': 0.3333333333333333}\n2 1 0.0 False {'prob': 0.3333333333333333}\n1 0 0.0 False {'prob': 0.3333333333333333}\n3 0 0.0 False {'prob': 0.3333333333333333}\n3 0 0.0 False {'prob': 0.3333333333333333}\n0 4 0.0 False {'prob': 0.3333333333333333}\n1 4 0.0 False {'prob': 0.3333333333333333}\n1 4 0.0 False {'prob': 0.3333333333333333}\n0 8 0.0 False {'prob': 0.3333333333333333}\n0 12 0.0 True {'prob': 0.3333333333333333}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Randomly explore the Grid for N times, where N = 100"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def randomExploreGridNTimes(N = 100):\n    print(\"starting\")\n    for i in range(N):\n        randomExploreGrid(False)\n    print (\"done\")      \nrandomExploreGridNTimes()       ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "starting\nsuccess\n1.0\nsuccess\n1.0\ndone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use Q-Learning instead of random exploration of the Grid"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def createQTable():\n    Qtable = np.zeros([env.observation_space.n,env.action_space.n])\n    lr = .8\n    gamma = .95\n    num_episodes = 2000\n    rList = []\n    for i in range(num_episodes):\n        s = env.reset()\n        rAll = 0\n        d = False\n        j = 0\n        while j < 99:\n            j+=1\n            a = np.argmax(Qtable[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n            s1,r,d,_ = env.step(a)\n            Qtable[s,a] = Qtable[s,a] + lr*(r + gamma*np.max(Qtable[s1,:]) - Qtable[s,a])\n            rAll += r\n            s = s1\n            if d == True:\n                break\n        rList.append(rAll)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}