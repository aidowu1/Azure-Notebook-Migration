{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Understanding how to cluster text data\n",
    "\n",
    "- This demo is a simplified version of sklearn original demo located here: http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us get some sample text data\n",
    "\n",
    "In this case I will get it from the sklearn import.\n",
    "- sklean import will provide the 'fetch_20newsgroups' object\n",
    "- Which loads the filenames and data from the 20 newsgroups dataset.\n",
    "- Also it is useful to debug the fetching of the data using logging, so I have added the logging import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to keep things simple here and I am just going to use 4 categories of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100  3661  100  3661    0     0   172k      0 --:--:-- --:--:-- --:--:--  188k\r\n"
     ]
    }
   ],
   "source": [
    "!curl \"http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz\" -o 20news-bydate.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "drwxr-xr-x 2 nbuser nbuser      0 Jan  1  1970 .\r\n",
      "drwx------ 7 nbuser nbuser   4096 Nov 11 13:17 ..\r\n",
      "drwxr-xr-x 2 nbuser nbuser      0 Jul 27 18:24 .ipynb_checkpoints\r\n",
      "-rw-r--r-- 1 nbuser nbuser  40960 Aug 23 00:24 .~Learning Feature Selection in Scikit Learn.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser   3661 Nov 11 14:07 20news-bydate.tar.gz\r\n",
      "-rw-r--r-- 1 nbuser nbuser   3394 Aug 22 19:12 Basic Into to K-Nearest Neighbors Algorithm in Scikit Learn.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser   1311 Nov 11 18:07 DemoSimpleCaching.py\r\n",
      "-rw-r--r-- 1 nbuser nbuser   8668 Nov 11 18:04 Demo_Clustering_Text_Data.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser   4997 Aug 16 00:26 Exploration_Of_Collaborative_Filtering.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser  27828 Jul 27 18:24 Exploration_Of_Logistic_Regression_Activation_Functions_Demo_Plot.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser  40960 Aug 22 17:44 Learning Feature Selection in Scikit Learn.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser   3490 Aug  3 08:54 Natural_Language_Processing_Intro_with_NLTK.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser     32 Aug 24 10:03 README.html\r\n",
      "-rw-r--r-- 1 nbuser nbuser 115522 Aug  5 13:09 Simple Stock Market Direction Prediction.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser  25422 Nov 11 13:06 Simple_Kmeans_Demo.ipynb\r\n",
      "-rw-r--r-- 1 nbuser nbuser   2651 Jul 27 18:24 Test_Azure_Notebook.ipynb\r\n",
      "drwxr-xr-x 2 nbuser nbuser      0 Nov 11 17:59 document_data\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing my simple api for reading the previously cached (picked) news group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -n DemoSimpleCaching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "# to show logging message of what is happening behind the scene.\n",
    "logging.basicConfig() \n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "#dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "#                             shuffle=True, random_state=42,download_if_missing=False)\n",
    "document_data_folder = r'./document_data'\n",
    "sc = SimpleCache(cache_file_folder=document_data_folder,cache_name='news_group_data')\n",
    "dataset = sc.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic facts of the news group dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 3387 documents\n",
      "Total categories are: 4\n",
      "List of unque labels are: [0 1 2 3]\n",
      "A sample of the News document data is shown here (for the sample, n = 1 out of 3387): \n",
      "\n",
      " From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: who are we to judge, Bobby?\n",
      "Lines: 38\n",
      "Organization: Walla Walla College\n",
      "Lines: 38\n",
      "\n",
      "In article <1993Apr14.213356.22176@ultb.isc.rit.edu> snm6394@ultb.isc.rit.edu (S.N. Mozumder ) writes:\n",
      ">From: snm6394@ultb.isc.rit.edu (S.N. Mozumder )\n",
      ">Subject\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.target\n",
    "unique_labels = np.unique(labels)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(\"There are: {} documents\".format(len(dataset.data)))\n",
    "print(\"Total categories are: {}\".format(len(dataset.target_names)))\n",
    "print(\"List of unque labels are: {}\".format(unique_labels))\n",
    "print(\"A sample of the News document data is shown here (for the sample, n = 1 out of 3387): \\n\\n {}\".\n",
    "      format(dataset.data[0][:300]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Term Frequency - Inverse Document Frequency (tf-idf)\n",
    "- This is a measure that can be used to idenfify/codify the importance of a word to a document in a collection or a corpus (collection of documents).\n",
    "\n",
    "- Further details can be found here: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
