{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering Documents Demo\n",
        "Reference: [DipanjanÂ Sarkar, Text Analytics with Python](https://www.apress.com/gb/book/9781484223871) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from os import path\n",
        "sys.path.append(r'..\\ClusteringDocumentsDemo')\n",
        "from normalization import normalize_corpus\n",
        "from utils import build_feature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "### Specify a random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "np.random.seed(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Define constants settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = r'..\\ClusteringDocumentsDemo'\n",
        "#HR_DATA_FILENAME = \"Start_doing_comments.csv\"\n",
        "HR_DATA_FILENAME = \"movie_data.csv\"\n",
        "HR_DOCS = path.join(DATA_PATH, HR_DATA_FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Corpus data from the source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadMoviesCorpusData():\n",
        "    movie_data = pd.read_csv(HR_DOCS)\n",
        "    print(movie_data.head())\n",
        "    movie_titles = movie_data['Title'].tolist()\n",
        "    movie_synopses = movie_data['Synopsis'].tolist()\n",
        "    print('Movie:{}'.format(movie_titles[0]))\n",
        "    print('Movie Synopsis:{}'.format(movie_synopses[0]))\n",
        "    return movie_synopses, movie_titles\n",
        "\n",
        "#movie_synopses, movie_titles = loadMoviesCorpusData()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalize the Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalize corpus\n",
        "norm_movie_synopses = normalize_corpus(movie_synopses,\n",
        "                                       lemmatize=True,\n",
        "                                       only_text_chars=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature_matrix.shape: (100, 307)\n",
            "feature_names[:20]: ['able', 'accept', 'across', 'act', 'agree', 'alive', 'allow', 'alone', 'along', 'already', 'although', 'always', 'another', 'anything', 'apartment', 'appear', 'approach', 'arm', 'army', 'around']\n"
          ]
        }
      ],
      "source": [
        "def computeTFIDFDocVectorization(norm_doc):\n",
        "    # extract tf-idf features\n",
        "    vectorizer, feature_matrix = build_feature_matrix(norm_movie_synopses,\n",
        "                                                  feature_type='tfidf',\n",
        "                                                  min_df=0.24, max_df=0.85,\n",
        "                                                  ngram_range=(1, 2))\n",
        "    return vectorizer, feature_matrix\n",
        "\n",
        "vectorizer, feature_matrix = computeTFIDFDocVectorization(norm_movie_synopses)\n",
        "# view number of features\n",
        "print(\"feature_matrix.shape: {}\".format(feature_matrix.shape))     \n",
        "\n",
        "# get feature names\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# print sample features\n",
        "print(\"feature_names[:20]: {}\".format(feature_names[:20]))      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply K-Means Clustering algorithm\n",
        "    - Cluster the data using K-means algorithm (based on a guessed number of clusters) to group the data into categorises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KMeansClustering(object):\n",
        "    \"\"\"\n",
        "    Cluster the data using K-means algorithm (based on a guessed number of clusters) to group the data into categorises\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def computeClusters(X_train, num_clusters=5, kmeans_type='classic'):\n",
        "        model = None\n",
        "        if kmeans_type == 'classic':\n",
        "            model = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=10000, n_init=1,\n",
        "                    verbose=is_verbose)\n",
        "        else:\n",
        "            model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1,\n",
        "                             init_size=1000, batch_size=1000, verbose=is_verbose)\n",
        "        model.fit(X_train)\n",
        "        clusters = model.labels_\n",
        "        return model, clusters\n",
        "    \n",
        "    @staticmethod\n",
        "    def predictClusters(model, X_valid):\n",
        "        predicted_clusters = km.predict(X_Valid)\n",
        "        return predicted_clusters\n",
        "    \n",
        "    @staticmethod\n",
        "    def computeNumDataPointsPerCluster(km_clusters):\n",
        "        c = Counter(km_clusters)\n",
        "        return c.items()\n",
        "    \n",
        "    @staticmethod\n",
        "    def getClusteredData(model, data, \n",
        "                     feature_names, num_clusters,\n",
        "                     topn_features=10):\n",
        "\n",
        "        cluster_details = {}  \n",
        "        # get cluster centroids\n",
        "        ordered_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "        # get key features for each cluster\n",
        "        # get movies belonging to each cluster\n",
        "        for cluster_num in range(num_clusters):\n",
        "            cluster_details[cluster_num] = {}\n",
        "            cluster_details[cluster_num]['cluster_num'] = cluster_num\n",
        "            key_features = [feature_names[index] \n",
        "                            for index \n",
        "                            in ordered_centroids[cluster_num, :topn_features]]\n",
        "            cluster_details[cluster_num]['key_features'] = key_features\n",
        "\n",
        "            data_point = data[movie_data['Cluster'] == cluster_num]['ID'].values.tolist()\n",
        "            cluster_details[cluster_num]['data_point'] = data_point\n",
        "\n",
        "        return cluster_details\n",
        "    \n",
        "    @staticmethod\n",
        "    def displayClusteredData(cluster_data):\n",
        "        \"\"\"\n",
        "        Display cluster details\n",
        "        \"\"\"\n",
        "        for cluster_num, cluster_details in cluster_data.items():\n",
        "            print('Cluster {} details:'.format(cluster_num))\n",
        "            print('-'*20)\n",
        "            print('Key features: {}'.format(cluster_details['key_features']))\n",
        "            print('Movies in this cluster:')\n",
        "            print(\"{}\".format( ', '.join(cluster_details['movies'])))\n",
        "            print('='*40)\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Loop of the Data Clustering Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def mainLoop():\n",
        "    np.random.seed(100)\n",
        "    total_num_clusters = 12\n",
        "    guess_num_clusters = 5\n",
        "    is_reduce_dim = False    \n",
        "    kmeans_algo_type = 'classic'\n",
        "    num_train_samples = 600\n",
        "    t0 = time()\n",
        "    print(\"Starting Clustering Steps..\\n\\n\")\n",
        "    print(\"1. Load the Documents from the disc\")\n",
        "    movie_synopses, movie_titles = loadMoviesCorpusData()\n",
        "    \n",
        "    print(\"2. Process the documents text data and vectorize it to a numeric matrix\")\n",
        "    X, vectorizer = processDoTextAndVectorizeData(docs)\n",
        "    print(\"3. Optionally, improve the performance of the unsupervised learning by reducing data dimensionality\")\n",
        "    if is_reduce_dim:\n",
        "        reduceDataDimensionality(X)\n",
        "    print(\"4. Split data into train and validation sets\")\n",
        "    X_train = X[:num_train_samples]\n",
        "    X_valid = X[num_train_samples:]\n",
        "    print(\"5. Cluster the data using K-means algorithm (based on a guessed number of clusters) to group the data into categorises\")\n",
        "    km = applyKmeansClustering(X_train, guess_num_clusters)\n",
        "    print(\"6. Report the Performance of the Clustering Algorithm on the Training Data using Silhouette Coefficient\")\n",
        "    reportClusteringPerformance(X_train, km)\n",
        "    print(\"7. Predict the Clusters running the tained model on unseen validation data\")\n",
        "    predicted_data = predictClusters(X_valid, km, docs[num_train_samples:])\n",
        "    print(\"predicted_data.head(10) = \\n{}\\n\".format(predicted_data.head(20)))\n",
        "    #print(tabulate(predicted_data.head(10), headers='keys', tablefmt='psql'))\n",
        "    print(\"8. Report the Top terms computed Cluster\")\n",
        "    reportTopTermsOfComputedClusters(km, vectorizer, guess_num_clusters)\n",
        "    t1 = time()\n",
        "    elasped_time = t1 - t0\n",
        "    print(\"Clustering process took {0:.3f} seconds\".format(elasped_time))\n",
        "    print(\"\")\n",
        "    findOptimalNumberOfClusters(X_train, total_num_clusters)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}